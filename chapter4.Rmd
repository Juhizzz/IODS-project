---
#Chapter4

First we load the "Boston" data from MASS package and explore it.
```{r}
library(MASS)
data("Boston")
str(Boston)
summary(Boston)
names(Boston)
```
"Boston":ssa on 506 havaintoa 14:sta muuttujasta. Names() funktio antaa muuttujien nimet. Summary() puolestaan tietoa jokaisen muuttujan jakaumasta erikseen.

Now we explore the relationship between variables.
```{r}
library(corrplot)
library(tidyverse)
cor_matrix<-cor(Boston)  %>% round(digits=2)
corrplot(cor_matrix, type="upper", cl.pos = "b",tl.pos = "d",tl.cex = 0.6  )

```
In the graph, blue colour represents positive correlation between variables and red negative ones. The darker the colour and the bigger the ball, the stronger the correlation is. For example, big blue ball between "rad" and "tax" tells that there exists a strong correlation between acces to highways and property tax rate.

4. 
Now we'll standardise the data
```{r}
boston_scaled <- scale(Boston)
summary(boston_scaled)
```
Now all the observations are using the same scale and for example making assumptions about summary is meaningful. 

Then turn boston_scaled back to being a data.frame
```{r}
boston_scaled <- as.data.frame(boston_scaled)
```

These lines give us a categorical variable crime
```{r}
bins <- quantile(boston_scaled$crim)
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
```

And these replace the old crim with the categorical crime
```{r}
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
```

Then we devide the data to train wiht 80% of the data and test with 20% of the data.
```{r}
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
```

5.
Now we'll create the LDA plot.
```{r}
lda.fit <- lda(crime ~ ., data = train)
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 1)

```
A very cool plot appears. 

6.
Here's the prediction of the LDA model on the test data.
```{r}
correct_classes <- test$crime
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
```
It appears that LDA model predicts the crime classes quite well.

7. 
Before we practice K-means clustering, we should reload the Boston data, scale it, and afterwards measure the distances between the observations. 

```{r}
data(Boston)
boston_scaled1<-as.data.frame(scale(Boston))
dist_eu<-dist(boston_scaled1)
summary(dist_eu)
head(boston_scaled1)
```


The scaled Boston data will now be used for K-means clustering. It isn't trivial (in many cases) to investigate on the number of clusters that can classify the data. Therefore, we need to first randomize the usage of a certain number of clusters.

First let's start with a random number cluster. Let us choose k=4 and apply k-means on the data.
```{r}
kmm = kmeans(boston_scaled1,6,nstart = 50 ,iter.max = 15) 
```



The elbow method is one good technique using which we can estimate the number of clusters.
```{r}
library(ggplot2)
set.seed(1234)
k.max <- 15
data <- boston_scaled1
wss <- sapply(1:k.max, 
              function(k){kmeans(data, k)$tot.withinss})
qplot(1:k.max, wss, geom = c("point", "line"), span = 0.2,
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")
```


  
The elbow plot seems to indicate that we may not find more than two clear clusters but it's good to confirm predictions using another method because there is no shortage of methods for analyses like these. Let's try the "NbClust"- package.
```{r}
library(NbClust)
nb <- NbClust(boston_scaled1, diss=NULL, distance = "euclidean", 
              min.nc=2, max.nc=5, method = "kmeans", 
              index = "all", alphaBeale = 0.1)
hist(nb$Best.nc[1,], breaks = max(na.omit(nb$Best.nc[1,])))
```


Now, it's easier to see that the data is described better with two clusters. With that, we should run the k-means algorithm again. 
```{r, fig.height=10, fig.width=10}
km_final = kmeans(boston_scaled1, centers = 2) 
pairs(boston_scaled1[3:9], col=km_final$cluster)
```




The clusters in the above plot are divided into two groups and outlined using the colors red and black. Some of the pairs are better grouped than other ones in the plot. One of the important observations can be made with the "chas"-variable where the observations in all of the pairs formed by it are wrongly clustered. Still, clusters formed by the "rad" variable are better separated.
